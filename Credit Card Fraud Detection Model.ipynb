{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5d47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cfd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fraudtrain and fraudtest are CSV files, we can read them into dataframes\n",
    "fraudtrain = pd.read_csv('fraudtrain.csv')\n",
    "fraudtest = pd.read_csv('fraudtest.csv')\n",
    "\n",
    "# Combine the datasets vertically (stack them on top of each other)\n",
    "credit_card_df = pd.concat([fraudtrain, fraudtest], ignore_index=True)\n",
    "\n",
    "credit_card_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = len(credit_card_df)\n",
    "fraudtrain_length = len(fraudtrain)\n",
    "fraudtest_length = len(fraudtest)\n",
    "\n",
    "print(\"fraudtrain Dataset length:\", fraudtrain_length)\n",
    "print(\"fraudtest Dataset length:\", fraudtest_length)\n",
    "print(\"credit card Dataset length:\", dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of columns in the DataFrame\n",
    "column_names = credit_card_df.columns\n",
    "# Print the column names\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(credit_card_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff095db",
   "metadata": {},
   "source": [
    "# Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2977f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column to remove\n",
    "column_to_remove = 'Unnamed: 0'\n",
    "# Remove the specified column\n",
    "credit_card_df = credit_card_df.drop(column_to_remove, axis=1)\n",
    "# Display the DataFrame after removing the column\n",
    "print('\\nDataFrame after removing the column \"Unnamed: 0\"')\n",
    "credit_card_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc490d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Name the index column\n",
    "credit_card_df = credit_card_df.rename_axis('Index ID')\n",
    "\n",
    "# Display the DataFrame after naming the index column\n",
    "credit_card_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current index of the DataFrame\n",
    "credit_card_df.shape\n",
    "print(credit_card_df.index)\n",
    "credit_card_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf0906",
   "metadata": {},
   "source": [
    "### Checking Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c510785",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = credit_card_df.isnull().sum()\n",
    "\n",
    "# Display missing values\n",
    "print(\"\\nTotal Missing Values in the Entire Dataset:\", missing_values.sum())\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60765bd",
   "metadata": {},
   "source": [
    "### Checking duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705263ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `duplicated()` function returns a boolean Series indicating duplicate rows\n",
    "duplicates = credit_card_df.duplicated()\n",
    "\n",
    "# Display duplicate rows\n",
    "duplicate_rows = [duplicates]\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows that are duplicates\n",
    "duplicate_rows = credit_card_df[credit_card_df.duplicated()]\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490e15c",
   "metadata": {},
   "source": [
    "### Checking data distribution of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6c63f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Select numerical columns for visualization\n",
    "numerical_columns = ['cc_num', 'amt', 'zip', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud']\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot histograms for numerical columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, column in enumerate(numerical_columns, 1):\n",
    "    plt.subplot(3, 4, i)\n",
    "    sns.histplot(credit_card_df[column], kde=True)\n",
    "    plt.title(f'Histogram - {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot box plots for numerical columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, column in enumerate(numerical_columns, 1):\n",
    "    plt.subplot(3, 4, i)\n",
    "    sns.boxplot(x=credit_card_df[column])\n",
    "    plt.title(f'Box Plot - {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot scatter plots for pairs of numerical columns (using a subsample)\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.pairplot(credit_card_df.sample(frac=0.1)[numerical_columns], hue='is_fraud', diag_kind='kde')\n",
    "plt.suptitle('Pair Plot for Numerical Columns (Subsample)', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76f0d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of categorical columns in the DataFrame\n",
    "categorical_columns = ['gender',  'category',  'state']\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot bar charts or count plots for each categorical column\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x=column, data=credit_card_df, palette='viridis')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54177991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "    'trans_date_trans_time', 'merchant', 'category', 'first', 'last',\n",
    "    'gender', 'street', 'city', 'state', 'job', 'dob'\n",
    "]\n",
    "\n",
    "for column in columns_to_check:\n",
    "    unique_values = credit_card_df[column].unique()\n",
    "    print(f\"Unique values in '{column}': {unique_values}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df['merchant'] = credit_card_df['merchant'].str.replace('fraud_', '')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(credit_card_df['merchant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7421f15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "credit_card_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b10907",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff748057",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 2: Display summary statistics for numeric columns\n",
    "numeric_summary = credit_card_df.describe()\n",
    "print(\"\\nSummary Statistics for Numeric Columns:\")\n",
    "print(numeric_summary)\n",
    "\n",
    "# Step 3: Calculate mean, median, and standard deviation for specific columns (e.g., 'amt')\n",
    "mean_amt = credit_card_df['amt'].mean()\n",
    "median_amt = credit_card_df['amt'].median()\n",
    "std_amt = credit_card_df['amt'].std()\n",
    "\n",
    "# Display the calculated values\n",
    "print(\"\\nMean Transaction Amount:\", mean_amt)\n",
    "print(\"Median Transaction Amount:\", median_amt)\n",
    "print(\"Standard Deviation of Transaction Amount:\", std_amt)\n",
    "\n",
    "# Step 4: Analyze summary statistics for the entire dataset\n",
    "# This will include non-numeric columns as well\n",
    "full_summary = credit_card_df.describe(include='all')\n",
    "print(\"\\nSummary Statistics for the Entire Dataset:\")\n",
    "print(full_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931730e3",
   "metadata": {},
   "source": [
    "### Finding and dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0b2d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List of numeric columns in the DataFrame\n",
    "numeric_columns = credit_card_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Set a common threshold for IQR (e.g., 1.5)\n",
    "iqr_threshold = 1.5\n",
    "\n",
    "# Dictionary to store outlier values for each numeric column\n",
    "outlier_values_dict = {}\n",
    "\n",
    "# Iterate through numeric columns\n",
    "for col in numeric_columns:\n",
    "    # Skip the target column 'is_fraud'\n",
    "    if col == 'is_fraud':\n",
    "        continue\n",
    "    \n",
    "    # Calculate IQR\n",
    "    Q1 = credit_card_df[col].quantile(0.25)\n",
    "    Q3 = credit_card_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - iqr_threshold * IQR\n",
    "    upper_bound = Q3 + iqr_threshold * IQR\n",
    "    \n",
    "    # Identify outliers based on IQR\n",
    "    outliers = (credit_card_df[col] < lower_bound) | (credit_card_df[col] > upper_bound)\n",
    "    \n",
    "    # Get the actual values of outliers\n",
    "    outlier_values = credit_card_df[col][outliers]\n",
    "    \n",
    "    # Store outlier values in the dictionary\n",
    "    outlier_values_dict[col] = outlier_values\n",
    "    \n",
    "    # Visualize the distribution and outliers using a boxplot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=credit_card_df[col])\n",
    "    plt.title(f'Boxplot for {col}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the identified outliers for each column\n",
    "    print(f\"Identified Outliers in '{col}' based on IQR:\")\n",
    "    print(outlier_values)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame to preserve the original data\n",
    "credit_card_df_no_outliers = credit_card_df.copy()\n",
    "\n",
    "# List of numeric columns in your DataFrame\n",
    "numeric_columns = credit_card_df_no_outliers.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Set a common threshold for IQR (e.g., 1.5)\n",
    "iqr_threshold = 1.5\n",
    "\n",
    "# Iterate through numeric columns\n",
    "for col in numeric_columns:\n",
    "    # Skip the target column 'is_fraud'\n",
    "    if col == 'is_fraud':\n",
    "        continue\n",
    "    \n",
    "    # Calculate IQR\n",
    "    Q1 = credit_card_df_no_outliers[col].quantile(0.25)\n",
    "    Q3 = credit_card_df_no_outliers[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - iqr_threshold * IQR\n",
    "    upper_bound = Q3 + iqr_threshold * IQR\n",
    "    \n",
    "    # Identify and remove outliers based on IQR\n",
    "    outliers = (credit_card_df_no_outliers[col] < lower_bound) | (credit_card_df_no_outliers[col] > upper_bound)\n",
    "    credit_card_df_no_outliers = credit_card_df_no_outliers[~outliers]\n",
    "\n",
    "# Display the shape of the new DataFrame without outliers\n",
    "print(\"Shape of DataFrame without outliers:\", credit_card_df_no_outliers.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab64e9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_credit_card_df_no_outliers_iqr=len(credit_card_df_no_outliers)\n",
    "new_credit_card_df=len(credit_card_df)\n",
    "print(\"credit_card_df_no_outliers_iqr: \",new_credit_card_df_no_outliers_iqr)\n",
    "print(\"credit_card_df: \", new_credit_card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75898a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target column in credit_card_df_no_outliers\n",
    "target_distribution_no_outliers = credit_card_df_no_outliers['is_fraud'].value_counts()\n",
    "# Check the distribution of the target column in credit_card_df\n",
    "target_distribution_credit_card_df = credit_card_df['is_fraud'].value_counts()\n",
    "\n",
    "\n",
    "# Display the distribution\n",
    "print(\"\\nTarget Distribution (Dataset without Outliers):\")\n",
    "print(target_distribution_no_outliers)\n",
    "\n",
    "# Display the distribution\n",
    "print(\"\\nTarget Distribution (Original Dataset):\")\n",
    "print(target_distribution_credit_card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97640c3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List of numeric columns in your DataFrame\n",
    "numeric_columns = credit_card_df_no_outliers.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Set the number of bins for histograms\n",
    "num_bins = 30\n",
    "\n",
    "# Iterate through numeric columns\n",
    "for col in numeric_columns:\n",
    "    # Visualize the distribution using a histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(credit_card_df_no_outliers[col], bins=num_bins, kde=True)\n",
    "    plt.title(f'Distribution of {col} (After Removing Outliers)')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d60611",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# List of numeric columns in your DataFrame\n",
    "numeric_columns = credit_card_df_no_outliers.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Set the number of bins for CDF plots\n",
    "num_bins_cdf = 100\n",
    "\n",
    "# Iterate through numeric columns\n",
    "for col in numeric_columns:\n",
    "    # Visualize the distribution using a CDF plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate ECDF\n",
    "    ecdf = ECDF(credit_card_df_no_outliers[col])\n",
    "    \n",
    "    # Create an array of values for the x-axis\n",
    "    x_values = np.linspace(min(credit_card_df_no_outliers[col]), max(credit_card_df_no_outliers[col]), num_bins_cdf)\n",
    "    \n",
    "    # Plot CDF\n",
    "    plt.step(x_values, ecdf(x_values), marker='o')\n",
    "    \n",
    "    plt.title(f'Cumulative Distribution Function (CDF) of {col} (After Removing Outliers)')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bece24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d5a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "fraud_data = credit_card_df_no_outliers[credit_card_df_no_outliers['is_fraud'] == 1]\n",
    "\n",
    "# Create a base map centered around the average latitude and longitude\n",
    "fraud_map = folium.Map(location=[fraud_data['lat'].mean(), fraud_data['long'].mean()], zoom_start=4)\n",
    "\n",
    "# Create a MarkerCluster layer for better visualization of multiple markers\n",
    "marker_cluster = MarkerCluster().add_to(fraud_map)\n",
    "\n",
    "# Add markers for each fraud entry\n",
    "for index, row in fraud_data.iterrows():\n",
    "    folium.Marker([row['lat'], row['long']], popup=f\"Fraud Transaction\\nAmount: ${row['amt']:.2f}\").add_to(marker_cluster)\n",
    "\n",
    "# Save or display the map\n",
    "fraud_map.save('fraud_map.html')  # Save the map as an HTML file\n",
    "fraud_map  # Display the map in Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacb3a4",
   "metadata": {},
   "source": [
    "## Data Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df_no_outliers['trans_date_trans_time'] = pd.to_datetime(credit_card_df_no_outliers['trans_date_trans_time'])\n",
    "credit_card_df_no_outliers['dob'] = pd.to_datetime(credit_card_df_no_outliers['dob'])\n",
    "# Verify the changes\n",
    "print(credit_card_df_no_outliers.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45425846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'trans_date_trans_time' and 'dob'\n",
    "missing_values_trans_time = credit_card_df['trans_date_trans_time'].isnull().sum()\n",
    "missing_values_dob = credit_card_df['dob'].isnull().sum()\n",
    "\n",
    "print(f\"Missing values in 'trans_date_trans_time': {missing_values_trans_time}\")\n",
    "print(f\"Missing values in 'dob': {missing_values_dob}\")\n",
    "\n",
    "# Check for duplicates in 'trans_date_trans_time' and 'dob'\n",
    "duplicates_trans_time = credit_card_df.duplicated(subset='trans_date_trans_time').sum()\n",
    "duplicates_dob = credit_card_df.duplicated(subset='dob').sum()\n",
    "\n",
    "print(f\"Duplicates in 'trans_date_trans_time': {duplicates_trans_time}\")\n",
    "print(f\"Duplicates in 'dob': {duplicates_dob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc014fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find and display rows with duplicated 'trans_date_trans_time'\n",
    "duplicated_trans_time_rows = credit_card_df[credit_card_df.duplicated(subset='trans_date_trans_time', keep=False)]\n",
    "print(f\"Rows with duplicated 'trans_date_trans_time':\")\n",
    "print(duplicated_trans_time_rows)\n",
    "\n",
    "# Find and display rows with duplicated 'dob'\n",
    "duplicated_dob_rows = credit_card_df[credit_card_df.duplicated(subset='dob', keep=False)]\n",
    "print(f\"Rows with duplicated 'dob':\")\n",
    "print(duplicated_dob_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40745858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the entire DataFrame\n",
    "duplicated_rows = credit_card_df[credit_card_df.duplicated(keep=False)]\n",
    "print(f\"Duplicate Rows in the Entire DataFrame:\")\n",
    "print(duplicated_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b699f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to check\n",
    "columns_to_check = ['category', 'gender', 'job', 'state']\n",
    "\n",
    "# Iterate through columns\n",
    "for col in columns_to_check:\n",
    "    unique_values = credit_card_df[col].unique()\n",
    "    value_counts = credit_card_df[col].value_counts()\n",
    "    \n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(\"Unique Values:\")\n",
    "    print(unique_values)\n",
    "    \n",
    "    print(\"\\nValue Counts:\")\n",
    "    print(value_counts)\n",
    "    \n",
    "    # Calculate the percentage of unique values\n",
    "    percentage_unique = (len(unique_values) / len(credit_card_df)) * 100\n",
    "    print(f\"\\nPercentage of Unique Values: {percentage_unique:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'category' column to categorical\n",
    "credit_card_df_no_outliers['category'] = credit_card_df_no_outliers['category'].astype('category')\n",
    "\n",
    "# Convert 'gender' column to categorical\n",
    "credit_card_df_no_outliers['gender'] = credit_card_df_no_outliers['gender'].astype('category')\n",
    "\n",
    "# Convert 'state' column to categorical\n",
    "credit_card_df_no_outliers['state'] = credit_card_df_no_outliers['state'].astype('category')\n",
    "\n",
    "# Check the data types after conversion\n",
    "print(credit_card_df_no_outliers.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ec641",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Convert 'dob' column to datetime\n",
    "credit_card_df_no_outliers['dob'] = pd.to_datetime(credit_card_df_no_outliers['dob'])\n",
    "\n",
    "# Calculate age based on the current date\n",
    "current_date = datetime.now()\n",
    "credit_card_df_no_outliers['age'] = (\n",
    "    pd.to_numeric(current_date - credit_card_df_no_outliers['dob']) / (365.25 * 24 * 60 * 60)\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(credit_card_df_no_outliers[['dob', 'age']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75eed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "column_order = ['first', 'last','gender', 'street', 'city', 'state','zip', 'lat', 'long','merch_lat', 'merch_long', 'city_pop', 'cc_num', 'amt','merchant','category','trans_date_trans_time', \n",
    "                'job', 'dob', 'age', 'trans_num', 'unix_time', 'is_fraud']\n",
    "\n",
    "\n",
    "# Update the DataFrame with the new column order\n",
    "credit_card_df_no_outliers = credit_card_df_no_outliers[column_order]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "credit_card_df_no_outliers.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the number of random rows we want to select\n",
    "num_rows_to_select = 10  # Change this number as needed\n",
    "\n",
    "# Randomly sample rows based on the 'unix_time' column\n",
    "random_rows = credit_card_df_no_outliers.sample(n=num_rows_to_select, random_state=42)\n",
    "\n",
    "# Display the randomly selected rows\n",
    "print(random_rows[['unix_time']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119aa381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# unix_time is the column containing Unix time\n",
    "credit_card_df_no_outliers['unix_time'] = pd.to_datetime(credit_card_df_no_outliers['unix_time'], unit='s')\n",
    "\n",
    "# Display the updated DataFrame with datetime format\n",
    "print(credit_card_df_no_outliers[['unix_time']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df_no_outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb99fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "credit_card_df_no_outliers['unix_time'] = pd.to_datetime(credit_card_df_no_outliers['unix_time'], unit='s')\n",
    "\n",
    "# Extract the hour of the day\n",
    "credit_card_df_no_outliers['transaction_hour'] = credit_card_df_no_outliers['unix_time'].dt.hour\n",
    "\n",
    "# Analyze fraud occurrences by hour\n",
    "fraud_by_hour = credit_card_df_no_outliers.groupby('transaction_hour')['is_fraud'].mean()\n",
    "\n",
    "# Assuming fraud_by_hour is our Series\n",
    "plt.figure(figsize=(10, 6))\n",
    "fraud_by_hour.plot(kind='bar', color='skyblue')\n",
    "plt.title('Fraud Occurrences by Hour')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Fraud Occurrence Rate')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Display the fraud occurrences by hour\n",
    "print(fraud_by_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df_no_outliers['unix_time'] = pd.to_datetime(credit_card_df_no_outliers['unix_time'], unit='s')\n",
    "\n",
    "# Extract the month and day of the week\n",
    "credit_card_df_no_outliers['transaction_month'] = credit_card_df_no_outliers['unix_time'].dt.month\n",
    "credit_card_df_no_outliers['day_of_week'] = credit_card_df_no_outliers['unix_time'].dt.dayofweek\n",
    "\n",
    "# Analyze fraud occurrences by month\n",
    "fraud_by_month = credit_card_df_no_outliers.groupby('transaction_month')['is_fraud'].mean()\n",
    "\n",
    "# Analyze fraud occurrences by day of the week\n",
    "fraud_by_day = credit_card_df_no_outliers.groupby('day_of_week')['is_fraud'].mean()\n",
    "\n",
    "# Plot Fraud Occurrences by Month\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=fraud_by_month.index, y=fraud_by_month.values, color='skyblue')\n",
    "plt.title('Fraud Occurrences by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Fraud Occurrence Rate')\n",
    "plt.show()\n",
    "\n",
    "# Plot Fraud Occurrences by Day of the Week\n",
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=days_of_week, y=fraud_by_day.values, color='lightcoral')\n",
    "plt.title('Fraud Occurrences by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Fraud Occurrence Rate')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Display the fraud occurrences by month and day of the week\n",
    "print(\"Fraud Occurrences by Month:\")\n",
    "print(fraud_by_month)\n",
    "\n",
    "print(\"\\nFraud Occurrences by Day of the Week:\")\n",
    "print(fraud_by_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit_card_df is our DataFrame and 'event_dates' is a list of dates corresponding to holidays or special events\n",
    "event_dates = ['2023-01-01', '2023-02-14', '2023-07-04']  # Add more dates as needed\n",
    "\n",
    "# Convert 'trans_date_trans_time' to datetime format\n",
    "credit_card_df['trans_date_trans_time'] = pd.to_datetime(credit_card_df['trans_date_trans_time'])\n",
    "\n",
    "# Create a new binary column indicating whether a transaction occurred on a holiday or special event\n",
    "credit_card_df['is_event'] = credit_card_df['trans_date_trans_time'].dt.date.astype(str).isin(event_dates).astype(int)\n",
    "\n",
    "# Analyze fraud occurrences during holidays or special events\n",
    "fraud_by_event = credit_card_df.groupby('is_event')['is_fraud'].mean()\n",
    "\n",
    "# Display the fraud occurrences during holidays or special events\n",
    "print(fraud_by_event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "# Transaction Hour\n",
    "credit_card_df_no_outliers['transaction_hour'] = credit_card_df_no_outliers['unix_time'].dt.hour\n",
    "\n",
    "# Transaction Day of the Week\n",
    "credit_card_df_no_outliers['day_of_week'] = credit_card_df_no_outliers['unix_time'].dt.dayofweek\n",
    "\n",
    "# Transaction Month\n",
    "credit_card_df_no_outliers['transaction_month'] = credit_card_df_no_outliers['unix_time'].dt.month\n",
    "\n",
    "# Transaction Amount Categories\n",
    "# You can customize the bins and labels based on the dataset\n",
    "bins = [0, 50, 200, np.inf]\n",
    "labels = ['small', 'medium', 'large']\n",
    "credit_card_df_no_outliers['amount_category'] = pd.cut(credit_card_df_no_outliers['amt'], \n",
    "                                                       bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Transaction Distance\n",
    "# Assuming we have 'lat' and 'long' for cardholder and merchant\n",
    "credit_card_df_no_outliers['distance_km'] = credit_card_df_no_outliers.apply(lambda row: geodesic((row['lat'], row['long']), (row['merch_lat'], row['merch_long'])).kilometers, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "credit_card_df_no_outliers.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b24c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'transaction_hour', 'day_of_week', and 'transaction_month' are the column names in your DataFrame\n",
    "unique_hours = credit_card_df_no_outliers['transaction_hour'].unique()\n",
    "unique_days_of_week = credit_card_df_no_outliers['day_of_week'].unique()\n",
    "unique_months = credit_card_df_no_outliers['transaction_month'].unique()\n",
    "\n",
    "print(\"Unique values in 'transaction_hour':\")\n",
    "print(unique_hours)\n",
    "\n",
    "print(\"\\nUnique values in 'day_of_week':\")\n",
    "print(unique_days_of_week)\n",
    "\n",
    "print(\"\\nUnique values in 'transaction_month':\")\n",
    "print(unique_months)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'distance_km' is the column name in our DataFrame\n",
    "credit_card_df_no_outliers['distance_km'] = credit_card_df_no_outliers['distance_km'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d17e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df_no_outliers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = ['first', 'last','gender', 'street', 'city', 'state','zip', 'lat', 'long','merch_lat', \n",
    "                'merch_long', 'city_pop', 'cc_num', 'amt','merchant','category','trans_date_trans_time', \n",
    "                'job', 'dob', 'age', 'trans_num', 'unix_time','transaction_hour','day_of_week','transaction_month'\n",
    "                ,'amount_category','distance_km', 'is_fraud']\n",
    "\n",
    "\n",
    "# Update the DataFrame with the new column order\n",
    "credit_card_df_no_outliers = credit_card_df_no_outliers[column_order]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "credit_card_df_no_outliers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d6137",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1ed59",
   "metadata": {},
   "source": [
    "### Demographic Analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot the distribution of transactions based on gender\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='gender', data=credit_card_df_no_outliers, hue='is_fraud')\n",
    "\n",
    "# Add data labels inside the chart\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.title('Distribution of Transactions Based on Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5115481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Increase the height of the figure\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "# Plot the distribution of transactions across different states\n",
    "ax = sns.countplot(y='state', data=credit_card_df_no_outliers, hue='is_fraud', orient='h')\n",
    "\n",
    "# Add data labels inside the chart\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_width()}', (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
    "                ha='left', va='center', xytext=(10, 0), textcoords='offset points')\n",
    "\n",
    "plt.title('Distribution of Transactions Across Different States')\n",
    "plt.xlabel('Transaction Count')\n",
    "plt.ylabel('State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acbf343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot the distribution of transactions across different states using a violin plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.violinplot(x='is_fraud', y='state', data=credit_card_df_no_outliers, inner='quartile')\n",
    "\n",
    "plt.title('Distribution of Transactions Across Different States')\n",
    "plt.xlabel('Fraudulent Transactions')\n",
    "plt.ylabel('State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf87ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Create a base map centered at an approximate location\n",
    "base_map = folium.Map(location=[37.7749, -122.4194], zoom_start=10)\n",
    "\n",
    "# Create a HeatMap layer using transaction amounts and city population\n",
    "heat_data = list(zip(credit_card_df_no_outliers['lat'], credit_card_df_no_outliers['long'], credit_card_df_no_outliers['amt']))\n",
    "HeatMap(heat_data).add_to(base_map)\n",
    "\n",
    "# display it\n",
    "base_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580d079",
   "metadata": {},
   "source": [
    "##### The correlation between transaction amounts and city population using a map, and create a scatter plot on a map where the color of each point is determined by the transaction amount, and the size of the point is determined by the city population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "subset_percentage = 0.1\n",
    "credit_card_df_subset = credit_card_df_no_outliers.sample(frac=subset_percentage, random_state=42)\n",
    "\n",
    "# Create a base map\n",
    "m = folium.Map(location=[credit_card_df_subset['lat'].mean(), credit_card_df_subset['long'].mean()], zoom_start=5)\n",
    "\n",
    "# Create a MarkerCluster layer\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# Add markers for each data point\n",
    "for index, row in credit_card_df_subset.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['long']],\n",
    "        radius=row['city_pop'] / 5000,  # Adjust the scale for better visualization\n",
    "        color='blue',  # Color of the marker\n",
    "        fill=True,\n",
    "        fill_color='blue',  # Fill color of the marker\n",
    "        fill_opacity=0.5,\n",
    "        popup=f\"Amount: {row['amt']}, Population: {row['city_pop']}\"\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f1b40",
   "metadata": {},
   "source": [
    "### Geographical Patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccca361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of frauds (is_fraud = 1) for each city\n",
    "total_frauds_by_location = credit_card_df.groupby(['city', 'state'])['is_fraud'].sum().reset_index()\n",
    "\n",
    "# Sort the DataFrame by the total number of frauds in descending order\n",
    "total_frauds_by_location = total_frauds_by_location.sort_values(by='is_fraud', ascending=False)\n",
    "\n",
    "# Plot the top N cities with the highest total number of frauds\n",
    "top_cities = 10  # You can adjust this based on your preference\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='is_fraud', y='city', data=total_frauds_by_location.head(top_cities))\n",
    "plt.title(f'Top {top_cities} Cities with Highest Total Number of Frauds')\n",
    "plt.xlabel('Total Number of Frauds')\n",
    "plt.ylabel('City')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1361f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of frauds (is_fraud = 1) for each state\n",
    "total_frauds_by_state = credit_card_df.groupby('state')['is_fraud'].sum().reset_index()\n",
    "\n",
    "# Sort the DataFrame by the total number of frauds in descending order\n",
    "total_frauds_by_state = total_frauds_by_state.sort_values(by='is_fraud', ascending=False)\n",
    "\n",
    "# Plot the top N states with the highest total number of frauds\n",
    "top_states = 10  # You can adjust this based on your preference\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='is_fraud', y='state', data=total_frauds_by_state.head(top_states))\n",
    "plt.title(f'Top {top_states} States with Highest Total Number of Frauds')\n",
    "plt.xlabel('Total Number of Frauds')\n",
    "plt.ylabel('State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212eecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for merchant locations with fraud highlighted\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = sns.scatterplot(x='merch_long', y='merch_lat', hue='is_fraud',\n",
    "                          data=credit_card_df, palette={0: 'blue', 1: 'red'}, alpha=0.5)\n",
    "plt.title('Merchant Locations with Fraud Highlighted')\n",
    "plt.xlabel('Merchant Longitude')\n",
    "plt.ylabel('Merchant Latitude')\n",
    "\n",
    "# Customize legend\n",
    "legend_labels = ['Not Fraud', 'Fraud']\n",
    "legend_handles = scatter.legend_.legendHandles\n",
    "plt.legend(legend_handles, legend_labels, title='Fraud', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d46ec1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for fraudulent transactions\n",
    "fraudulent_df = credit_card_df[credit_card_df['is_fraud'] == 1]\n",
    "\n",
    "# Group by merchants and count the number of fraudulent transactions\n",
    "merchant_fraud_counts = fraudulent_df['merchant'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 10 merchants\n",
    "top_merchants = merchant_fraud_counts.head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_merchants.values, y=top_merchants.index, palette='viridis')\n",
    "plt.title('Top 10 Merchants with Fraudulent Transactions')\n",
    "plt.xlabel('Number of Fraudulent Transactions')\n",
    "plt.ylabel('Merchant')\n",
    "plt.show()\n",
    "\n",
    "# Display the total number of fraudulent transactions\n",
    "total_fraudulent_transactions = len(fraudulent_df)\n",
    "print(f'Total Fraudulent Transactions: {total_fraudulent_transactions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter fraudulent transactions\n",
    "fraudulent_df = credit_card_df[credit_card_df['is_fraud'] == 1]\n",
    "\n",
    "# Count the number of fraudulent transactions per category\n",
    "fraudulent_category_counts = fraudulent_df['category'].value_counts().head(10)\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=fraudulent_category_counts.values, y=fraudulent_category_counts.index, palette='viridis')\n",
    "\n",
    "plt.title('Top 10 Categories with Most Fraudulent Transactions')\n",
    "plt.xlabel('Number of Fraudulent Transactions')\n",
    "plt.ylabel('Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns needed: 'distance_km' and 'is_fraud'\n",
    "\n",
    "# Scatter plot for distance vs. fraud\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='distance_km', y='is_fraud', data=credit_card_df_no_outliers, alpha=0.5)\n",
    "plt.title('Distance vs. Fraud')\n",
    "plt.xlabel('Distance (km)')\n",
    "plt.ylabel('Fraud (1: Fraud, 0: Not Fraud)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distance bins\n",
    "bins = pd.cut(credit_card_df_no_outliers['distance_km'], bins=10)\n",
    "\n",
    "# Calculate mean fraud rate for each distance bin\n",
    "fraud_rate_by_distance = credit_card_df_no_outliers.groupby(bins)['is_fraud'].mean().reset_index()\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='is_fraud', y='distance_km', data=fraud_rate_by_distance, color='skyblue')\n",
    "plt.title('Mean Fraud Rate by Distance Bins')\n",
    "plt.xlabel('Mean Fraud Rate')\n",
    "plt.ylabel('Distance (km)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a violin plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='is_fraud', y='distance_km', data=credit_card_df_no_outliers, palette='pastel', inner='quartile')\n",
    "plt.title('Distribution of Distance by Fraud')\n",
    "plt.xlabel('Fraud (1: Fraud, 0: Not Fraud)')\n",
    "plt.ylabel('Distance (km)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e9421",
   "metadata": {},
   "source": [
    "## Temporal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18410f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the style for seaborn plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot 1: Peak hours for transactions\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='transaction_hour', data=credit_card_df_no_outliers, palette='viridis')\n",
    "plt.title('Number of Transactions by Hour')\n",
    "plt.xlabel('Transaction Hour')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Fraud rates by hour\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='transaction_hour', y='is_fraud', data=credit_card_df_no_outliers, palette='magma')\n",
    "plt.title('Fraud Rates by Hour')\n",
    "plt.xlabel('Transaction Hour')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Day-of-week analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='day_of_week', data=credit_card_df_no_outliers, hue='is_fraud', palette='Set1')\n",
    "plt.title('Number of Transactions and Fraud Rates by Day of Week')\n",
    "plt.xlabel('Day of Week (0=Monday, 6=Sunday)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Seasonal patterns in fraud\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='transaction_month', data=credit_card_df_no_outliers, hue='is_fraud', palette='coolwarm')\n",
    "plt.title('Number of Transactions and Fraud Rates by Month')\n",
    "plt.xlabel('Transaction Month')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c5dbc",
   "metadata": {},
   "source": [
    "## Transaction Characteristics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd6d86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_fraudulent_merchants = credit_card_df_no_outliers[credit_card_df_no_outliers['is_fraud'] \n",
    "                                                      == 1]['merchant'].value_counts().head(10).index\n",
    "\n",
    "# Filter the DataFrame for the top 10 fraudulent merchants\n",
    "top_fraudulent_df = credit_card_df_no_outliers[credit_card_df_no_outliers['merchant'].isin(top_fraudulent_merchants)]\n",
    "\n",
    "# Set the style for seaborn plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot 1: Merchants and categories involved in fraud for top 10 fraudulent merchants\n",
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.countplot(x='merchant', data=top_fraudulent_df, hue='is_fraud', palette='viridis')\n",
    "\n",
    "# Add data labels inside the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=10, color='black')\n",
    "\n",
    "plt.title('Number of Transactions and Fraud Rates by Top 10 Fraudulent Merchants')\n",
    "plt.xlabel('Merchant')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700bf98c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the style for seaborn plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "# Plot 1: Correlation between cardholder age and transaction amounts\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='age', y='amt', data=credit_card_df_no_outliers, hue='is_fraud', palette='coolwarm', alpha=0.5)\n",
    "plt.title('Correlation between Cardholder Age and Transaction Amounts')\n",
    "plt.xlabel('Cardholder Age')\n",
    "plt.ylabel('Transaction Amount')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.legend(loc=\"best\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for fraudulent transactions\n",
    "fraudulent_data = credit_card_df_no_outliers[credit_card_df_no_outliers['is_fraud'] == 1]\n",
    "\n",
    "# Get the top 10 job roles based on median transaction amount for fraudulent transactions\n",
    "top_10_fraudulent_jobs = fraudulent_data.groupby('job')['amt'].median().sort_values(ascending=False).head(10).index\n",
    "\n",
    "# Filter the data for the top 10 fraudulent job roles\n",
    "top_10_fraudulent_data = credit_card_df_no_outliers[credit_card_df_no_outliers['job'].isin(top_10_fraudulent_jobs)]\n",
    "\n",
    "# Plot the boxplot\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='job', y='amt', data=top_10_fraudulent_data, hue='is_fraud', palette='Pastel1')\n",
    "plt.title('Transaction Amounts and Fraud by Top 10 Job Roles')\n",
    "plt.xlabel('Job Role')\n",
    "plt.ylabel('Transaction Amount')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df_no_outliers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fb028",
   "metadata": {},
   "source": [
    "## Apply ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into majority and minority classes\n",
    "majority_class = credit_card_df_no_outliers[credit_card_df_no_outliers['is_fraud'] == 0]\n",
    "minority_class = credit_card_df_no_outliers[credit_card_df_no_outliers['is_fraud'] == 1]\n",
    "\n",
    "# Define the desired number of rows for each class\n",
    "desired_rows_majority = len(minority_class) * 2  # Assuming you want a 2:1 ratio\n",
    "\n",
    "# Downsample the majority class to match the desired number of rows\n",
    "downsampled_majority_class = majority_class.sample(n=desired_rows_majority, random_state=42)\n",
    "\n",
    "# Concatenate the downsampled majority class with the minority class\n",
    "new_balanced_credit_card_df = pd.concat([downsampled_majority_class, minority_class])\n",
    "\n",
    "# Shuffle the new DataFrame\n",
    "new_balanced_credit_card_df = new_balanced_credit_card_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print the distribution of the target variable in the new balanced dataset\n",
    "print(\"Target Distribution (New Balanced Dataset):\")\n",
    "print(new_balanced_credit_card_df['is_fraud'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1979a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_columns = new_balanced_credit_card_df.select_dtypes(include='number')\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "# Print correlation values\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Create a heatmap for visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Data Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864be07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_balanced_credit_card_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f50d54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'new_balanced_credit_card_df' is our DataFrame\n",
    "target_column = 'is_fraud'\n",
    "\n",
    "# Feature engineering for datetime columns\n",
    "new_balanced_credit_card_df['trans_hour'] = new_balanced_credit_card_df['trans_date_trans_time'].dt.hour\n",
    "new_balanced_credit_card_df['trans_day'] = new_balanced_credit_card_df['trans_date_trans_time'].dt.day\n",
    "new_balanced_credit_card_df['trans_month'] = new_balanced_credit_card_df['trans_date_trans_time'].dt.month\n",
    "new_balanced_credit_card_df['dob_year'] = new_balanced_credit_card_df['dob'].dt.year\n",
    "\n",
    "# Extract features and target variable\n",
    "X = new_balanced_credit_card_df.drop([target_column, 'trans_date_trans_time', 'dob', 'unix_time'], axis=1)\n",
    "y = new_balanced_credit_card_df[target_column]\n",
    "\n",
    "# Convert categorical columns to numerical using Label Encoding\n",
    "le = LabelEncoder()\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_columns:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print or display the feature importance DataFrame\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d50db",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Features and target variable\n",
    "X = new_balanced_credit_card_df.drop(columns=['is_fraud'])\n",
    "y = new_balanced_credit_card_df['is_fraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numeric_features = ['zip', 'lat', 'long', 'merch_lat', 'merch_long', 'city_pop', 'cc_num', 'amt', 'transaction_hour',\n",
    "                    'day_of_week', 'transaction_month', 'distance_km', 'age']\n",
    "categorical_features = ['gender', 'state', 'category', 'amount_category']\n",
    "\n",
    "# Preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', rf_classifier)])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 5, 10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_pipeline = grid_search.best_estimator_\n",
    "# Make predictions\n",
    "y_pred = best_rf_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=['Not Fraud', 'Fraud'])\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_pred_proba = best_rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"ROC AUC for RF: {roc_auc:.2f}\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Confusion Matrix as a Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ebf1a",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d04845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "\n",
    "# Convert datetime features\n",
    "new_balanced_credit_card_df['trans_date_trans_time'] = pd.to_datetime(new_balanced_credit_card_df['trans_date_trans_time'])\n",
    "new_balanced_credit_card_df['dob'] = pd.to_datetime(new_balanced_credit_card_df['dob'])\n",
    "new_balanced_credit_card_df['transaction_hour'] = new_balanced_credit_card_df['trans_date_trans_time'].dt.hour\n",
    "new_balanced_credit_card_df['day_of_week'] = new_balanced_credit_card_df['trans_date_trans_time'].dt.dayofweek\n",
    "new_balanced_credit_card_df['transaction_month'] = new_balanced_credit_card_df['trans_date_trans_time'].dt.month\n",
    "\n",
    "# Selecting features\n",
    "numeric_features = ['zip', 'lat', 'long', 'merch_lat', 'merch_long', 'city_pop', 'cc_num', 'amt', 'transaction_hour', 'day_of_week', 'transaction_month', 'distance_km', 'age']\n",
    "categorical_features = ['gender', 'state', 'category', 'amount_category']\n",
    "\n",
    "# Target variable\n",
    "target = 'is_fraud'\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    new_balanced_credit_card_df.drop(columns=[target]),\n",
    "    new_balanced_credit_card_df[target],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=new_balanced_credit_card_df[target]  # Ensures balanced distribution in train and test sets\n",
    ")\n",
    "\n",
    "# Preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combining transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Creating the SVM model\n",
    "svm_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('classifier', SVC(probability=True))])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10, 100],\n",
    "    'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'classifier__gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_svm = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params_svm = grid_search_svm.best_params_\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = best_svm_model.predict(X_test)\n",
    "\n",
    "# Get predicted probabilities for ROC curve\n",
    "y_probs_svm = best_svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "classification_rep_svm = classification_report(y_test, y_pred_svm, target_names=['Not Fraud', 'Fraud'])\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_probs_svm)\n",
    "roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Hyperparameters for SVM:\", best_params_svm)\n",
    "print(f\"Accuracy for SVM: {accuracy_svm:.2f}\")\n",
    "print(\"Confusion Matrix for SVM:\")\n",
    "print(conf_matrix_svm)\n",
    "print(\"Classification Report for SVM:\")\n",
    "print(classification_rep_svm)\n",
    "print(f\"ROC AUC for SVM: {roc_auc_svm:.2f}\")\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
    "plt.title('Confusion Matrix for SVM')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Plotting ROC Curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_svm, tpr_svm, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc_svm:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for SVM')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e2433",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "\n",
    "# Create a new DataFrame for logistic regression\n",
    "logreg_df = new_balanced_credit_card_df.copy()\n",
    "\n",
    "# Convert the target variable to binary\n",
    "logreg_df['is_fraud'] = logreg_df['is_fraud'].astype('bool')\n",
    "\n",
    "# Convert datetime features\n",
    "logreg_df['trans_date_trans_time'] = pd.to_datetime(logreg_df['trans_date_trans_time'])\n",
    "logreg_df['dob'] = pd.to_datetime(logreg_df['dob'])\n",
    "logreg_df['transaction_hour'] = logreg_df['trans_date_trans_time'].dt.hour\n",
    "logreg_df['day_of_week'] = logreg_df['trans_date_trans_time'].dt.dayofweek\n",
    "logreg_df['transaction_month'] = logreg_df['trans_date_trans_time'].dt.month\n",
    "\n",
    "# Selecting features\n",
    "numeric_features = ['zip', 'lat', 'long', 'merch_lat', 'merch_long', 'city_pop', 'cc_num', 'amt', 'transaction_hour',\n",
    "                    'day_of_week', 'transaction_month', 'distance_km', 'age']\n",
    "categorical_features = ['gender', 'state', 'category', 'amount_category']\n",
    "\n",
    "# Target variable\n",
    "target = 'is_fraud'\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    logreg_df.drop(columns=[target]),\n",
    "    logreg_df[target],\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=logreg_df[target]  # Ensures balanced distribution in train and test sets\n",
    ")\n",
    "\n",
    "# Preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combining transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Creating the Logistic Regression model\n",
    "logreg_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('classifier', LogisticRegression(random_state=42, solver='lbfgs', max_iter=1000))])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l2']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_logreg = GridSearchCV(logreg_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params_logreg = grid_search_logreg.best_params_\n",
    "best_logreg_model = grid_search_logreg.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logreg = best_logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "conf_matrix_logreg = confusion_matrix(y_test, y_pred_logreg)\n",
    "classification_rep_logreg = classification_report(y_test, y_pred_logreg, target_names=['Not Fraud', 'Fraud'])\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr_logreg, tpr_logreg, thresholds_logreg = roc_curve(y_test, best_logreg_model.predict_proba(X_test)[:, 1])\n",
    "roc_auc_logreg = auc(fpr_logreg, tpr_logreg)\n",
    "\n",
    "# Plotting the ROC Curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_logreg, tpr_logreg, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc_logreg:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Confusion Matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(conf_matrix_logreg, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], \n",
    "            yticklabels=['Not Fraud', 'Fraud'])\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Hyperparameters for Logistic Regression:\", best_params_logreg)\n",
    "print(f\"Accuracy for Logistic Regression: {accuracy_logreg:.2f}\")\n",
    "print(\"Confusion Matrix for Logistic Regression:\")\n",
    "print(conf_matrix_logreg)\n",
    "print(\"Classification Report for Logistic Regression:\")\n",
    "print(classification_rep_logreg)\n",
    "print(f\"ROC AUC for Logistic Regression: {roc_auc_logreg:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f1230",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f08ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Create a new DataFrame for the decision tree\n",
    "tree_df = new_balanced_credit_card_df.copy()\n",
    "\n",
    "# Convert the target variable to binary\n",
    "tree_df['is_fraud'] = tree_df['is_fraud'].astype('bool')\n",
    "\n",
    "# Convert datetime features\n",
    "tree_df['trans_date_trans_time'] = pd.to_datetime(tree_df['trans_date_trans_time'])\n",
    "tree_df['dob'] = pd.to_datetime(tree_df['dob'])\n",
    "tree_df['transaction_hour'] = tree_df['trans_date_trans_time'].dt.hour\n",
    "tree_df['day_of_week'] = tree_df['trans_date_trans_time'].dt.dayofweek\n",
    "tree_df['transaction_month'] = tree_df['trans_date_trans_time'].dt.month\n",
    "\n",
    "# Selecting features\n",
    "numeric_features = ['zip', 'lat', 'long', 'merch_lat', 'merch_long', 'city_pop', 'cc_num', 'amt', 'transaction_hour', 'day_of_week', 'transaction_month', 'distance_km', 'age']\n",
    "categorical_features = ['gender', 'state', 'category', 'amount_category']\n",
    "\n",
    "# Target variable\n",
    "target = 'is_fraud'\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tree_df.drop(columns=[target]),\n",
    "    tree_df[target],\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=tree_df[target]  # Ensures balanced distribution in train and test sets\n",
    ")\n",
    "\n",
    "# Custom transformer for selecting numeric or categorical features\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.feature_names]\n",
    "\n",
    "# Creating and training the Decision Tree model\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Creating the final pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', FeatureSelector(numeric_features), numeric_features),\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('selector', FeatureSelector(categorical_features)),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_features)\n",
    "        ])),\n",
    "    ('classifier', tree_model)\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {'classifier__max_depth': [None, 5, 10, 15], 'classifier__min_samples_split': [2, 5, 10]}\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_max_depth = grid_search.best_params_['classifier__max_depth']\n",
    "best_min_samples_split = grid_search.best_params_['classifier__min_samples_split']\n",
    "print(f\"Best Max Depth: {best_max_depth}\")\n",
    "print(f\"Best Min Samples Split: {best_min_samples_split}\")\n",
    "\n",
    "# Create and train the final Decision Tree model with the best hyperparameters\n",
    "final_tree_model = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', FeatureSelector(numeric_features), numeric_features),\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('selector', FeatureSelector(categorical_features)),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_features)\n",
    "        ])),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        max_depth=best_max_depth,\n",
    "        min_samples_split=best_min_samples_split,\n",
    "        random_state=42))\n",
    "])\n",
    "final_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = final_tree_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=['Not Fraud', 'Fraud'])\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, final_tree_model.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot Confusion Matrix with values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add values in each cell\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.xticks([0, 1], ['Predicted Not Fraud', 'Predicted Fraud'])\n",
    "plt.yticks([0, 1], ['Actual Not Fraud', 'Actual Fraud'])\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd47cf",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2affb04b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     46\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextwrap\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m operators\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asserts\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\operators\\__init__.py:36\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"This module implements operators that AutoGraph overloads.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mNote that \"operator\" is used loosely here, and includes control structures like\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mconditionals and loops, implemented in functional form, using for example\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mclosures for the body.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Naming conventions:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#  * operator names match the name usually used for the respective Python\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#    idiom; examples: for_stmt, list_append\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# subclasses namedtuple and contains any arguments that are only required\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# for some specializations of the operator.\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconditional_expressions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m if_exp\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrol_flow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m for_stmt\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrol_flow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m if_stmt\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\operators\\conditional_expressions.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Conditional expressions (e.g. the ternary if statement).\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensors\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cond \u001b[38;5;28;01mas\u001b[39;00m tf_cond\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:65\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_builtins\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variables\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m misc\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1612\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Features and target variable\n",
    "X = new_balanced_credit_card_df.drop(columns=['is_fraud'])\n",
    "y = new_balanced_credit_card_df['is_fraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numeric_features = ['zip', 'lat', 'long', 'merch_lat', 'merch_long', 'city_pop', 'cc_num', 'amt', 'transaction_hour',\n",
    "                    'day_of_week', 'transaction_month', 'distance_km', 'age']\n",
    "categorical_features = ['gender', 'state', 'category', 'amount_category']\n",
    "\n",
    "# Preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to training and testing sets\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Assuming X_train_preprocessed is your sparse matrix\n",
    "X_train_preprocessed_np = X_train_preprocessed.toarray()\n",
    "\n",
    "# Build a simple neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_preprocessed_np.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_preprocessed_np, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_preprocessed_np = X_test_preprocessed.toarray()\n",
    "accuracy = model.evaluate(X_test_preprocessed_np, y_test)[1]\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_preprocessed_np)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "classification_rep = classification_report(y_test, y_pred_binary, target_names=['Not Fraud', 'Fraud'])\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot Confusion Matrix with values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add values in each cell\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.xticks([0, 1], ['Predicted Not Fraud', 'Predicted Fraud'])\n",
    "plt.yticks([0, 1], ['Actual Not Fraud', 'Actual Fraud'])\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cd4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
